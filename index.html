<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A new benchmark for evaluating real-world few-shot reasoning for machine vision.">
  <meta name="keywords" content="Bongard Problems, Few-shot learning, Visual reasoning, Open world learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Bongard-OpenWorld</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Bongard-OpenWorld: Few-Shot Reasoning <br> for Free-form Visual Concepts in the Real World</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://rujiewu.github.io/">Rujie Wu</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://jeasinema.github.io/">Xiaojian Ma</a><sup>*2</sup>,</span>
            <span class="author-block">
              <a href="https://www.zlz.link/">Zhenliang Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://cognn.com/">Wei Wang</a><sup>†2</sup>,
            </span>
            <span class="author-block">
              <a href="https://liqing-ustc.github.io/">Qing Li</a><sup>†2</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhusongchun.net/">Song-Chun Zhu</a><sup>2,3,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm">Yizhou Wang</a><sup>1,4</sup>
            </span>
          </div>

          <div>
            <span class="author-block" style="display: block; margin-bottom: 5px;"><sup>1</sup>School of Computer Science, Peking University</span>
            <span class="author-block" style="display: block; margin-bottom: 5px;"><sup>2</sup>National Key Laboratory of General Artificial Intelligence, BIGAI</span>
            <span class="author-block" style="display: block; margin-bottom: 5px;"><sup>3</sup>School of Intelligence Science and Technology, Peking University</span>
            <span class="author-block" style="display: block; margin-bottom: 5px;"><sup>4</sup>Institute for Artificial Intelligence, Peking University</span>
            <span class="author-block" style="display: block; margin-bottom: 5px;"><sup>*</sup>Equal contribution (Accepted to ICLR 2024)&emsp;&emsp;<sup>†</sup>Co-corresponding authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.10207"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Paper Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2310.10207.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rujiewu/Bongard-OpenWorld"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/rujiewu/Bongard-OpenWorld"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <img src="static/images/teaser.jpg"/>
        <div class="content has-text-justified">
          <p style="text-align:justify; margin-top: 1.5em;">
            We introduce <b>Bongard-OpenWorld</b>, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical <em>Bongard Problems (BPs)</em> : Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs & VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.
          </p>
        </div>
      </div>
    </div>

    <!-- Approaches. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approaches</h2>
        <img src="static/images/method.jpg"/>
        <div class="content has-text-justified">
          <p style="text-align:justify; margin-bottom: 1.5em; margin-top: 1.5em;">
            We explore four families of approaches: (a) casting Bongard-OpenWorld into a standard ''2-way, 6-shot'' few-shot learning problem and tackling it using state-of-the-art few-shot learners with pretrained image representations; (b) combining an LLM (reasoner) and a VLM (image captioner) in a single round fashion, where the VLM simply caption each Bongard image and send their captions to LLM for solving this problem; (c) extending the method in (b) to multiple rounds, where the LLM will also iteratively probe the VLM for more image details, resulting in more condense information for solving Bongard; (d) neuro-symbolic approach, where a VLM generates the initial captions, then an LLM extracts visual concepts from them. These concepts are subsequently updated through logical operations, leveraging the responses provided by VLM, until the problem is solved. Zoom in for a better view.
          </p>
        </div>
      </div>
    </div>

    <!-- Qualitative Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <section class="hero is-light is-small">
          <div class="hero-body">
            <div class="container">
              <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-steve">
                  <img src="./static/images/qual_1.jpg">
                </div>
                <div class="item item-chair-tp">
                  <img src="./static/images/qual_2.jpg">
                </div>
                <div class="item item-shiba">
                  <img src="./static/images/qual_3.jpg">
                </div>
                <div class="item item-fullbody">
                  <img src="./static/images/qual_4.jpg">
                </div>
                <div class="item item-fullbody">
                  <img src="./static/images/qual_5.jpg">
                </div>
                <div class="item item-fullbody">
                  <img src="./static/images/qual_6.jpg">
                </div>
                <div class="item item-fullbody">
                  <img src="./static/images/qual_7.jpg">
                </div>
              </div>
            </div>
          </div>
        </section>
        <div class="content has-text-justified">
          <p style="text-align:justify; margin-bottom: 1.5em; margin-top: 1.5em;">
            Here are some remarkable qualitative results that demonstrate the reasoning capabilities exhibited by different approaches. For a better view, please refer to our <a href="https://arxiv.org/pdf/2310.10207.pdf">Paper</a>, compelling quantitative results can be found in the Table 3.
          </p>
        </div>
      </div>
    </div>

    <!-- Examples. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Examples</h2>
        <section class="hero is-light is-small">
          <div class="hero-body">
            <div class="container">
              <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-steve">
                  <img src="./static/images/example_1.jpg">
                </div>
                <div class="item item-chair-tp">
                  <img src="./static/images/example_2.jpg">
                </div>
                <div class="item item-shiba">
                  <img src="./static/images/example_3.jpg">
                </div>
                <div class="item item-fullbody">
                  <img src="./static/images/example_4.jpg">
                </div>
                <div class="item item-blueshirt">
                  <img src="./static/images/example_5.jpg">
                </div>
              </div>
            </div>
          </div>
        </section>
        <div class="content has-text-justified">
          <p style="text-align:justify; margin-bottom: 1.5em; margin-top: 1.5em;">
            Here are some examples of Bongard-OpenWorld, if you want to see more, please refer to our <a href="https://huggingface.co/datasets/rujiewu/Bongard-OpenWorld">Dataset</a>.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{wu2024bongardopenworld,
      title={Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World}, 
      author={Rujie Wu and Xiaojian Ma and Zhenliang Zhang and Wei Wang and Qing Li and Song-Chun Zhu and Yizhou Wang},
      year={2024},
      eprint={2310.10207},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2310.10207.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/rujiewu/Bongard-OpenWorld" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was adopted from <a href="https://nerfies.github.io/">Nerfies</a>. Send feedback and questions to <a href="https://rujiewu.github.io/">Rujie Wu</a> and <a href="https://jeasinema.github.io/">Xiaojian Ma</a>.
          </p>
          <!-- <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
